---
title: 'Deep Learning Over-Parameterization: the Shallow Fallacy'
openreview: ojWMOJUDtP
abstract: 'A major tenet of conventional wisdom dictates that models should not be
  over-parameterized: the number of free parameters should not exceed the number of
  training data points. This tenet originates from centuries of shallow learning,
  primarily in the form of linear or logistic regression. It is routinely applied
  to all kinds of data analyses and modeling and even to infer properties of the brain.
  However, through a variety of precise mathematical examples, we show that this conventional
  wisdom is completely wrong as soon as one moves from shallow to deep learning. In
  particular, we construct sequences of both linear and non-linear deep learning models
  whose number of parameters can grow to infinity, while the training set can remain
  very small (e.g. a single example). In deep models, the parameter space is partitioned
  into large equivalence classes. Learning can be viewed as a communication process
  where information is communicated from the data to the synaptic weights. The information
  in the training data only needs to specify an equivalence class of the parameters,
  and not the exact parameter values. As such, the number of training examples can
  be significantly smaller than the number of free parameters.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: baldi24a
month: 0
tex_title: 'Deep Learning Over-Parameterization: the Shallow Fallacy'
firstpage: 7
lastpage: 12
page: 7-12
order: 7
cycles: false
bibtex_author: Baldi, Pierre
author:
- given: Pierre
  family: Baldi
date: 2024-01-23
address:
container-title: Proceedings of the 5th Northern Lights Deep Learning Conference ({NLDL})
volume: '233'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 1
  - 23
pdf: https://proceedings.mlr.press/v233/baldi24a/baldi24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
